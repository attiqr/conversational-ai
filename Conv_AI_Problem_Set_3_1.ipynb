{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y71bnIOFL-Je",
        "outputId": "6ac32464-9071-480c-f26a-5b3e958e2337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Step 1: Install NLTK\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import the word_tokenize function\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "4VPVJF1GL-09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a variable 'text' with a paragraph\n",
        "text = \"\"\"\n",
        "Artificial Intelligence (AI) is revolutionizing the way we live and work.\n",
        "From self-driving cars to virtual assistants, AI technologies are becoming increasingly prevalent.\n",
        "These innovations are not just enhancing productivity but also improving the quality of life.\n",
        "As AI continues to evolve, it raises important ethical and social questions that society must address.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "VT2-FSyjMGWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Use the word_tokenize function to tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "# Tokenizes the paragraph into a list of words."
      ],
      "metadata": {
        "id": "zS5-V7nVMJCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Print the list of tokens\n",
        "print(\"List of tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4GBd5e1MMCA",
        "outputId": "ccf08843-f80e-4142-ef9f-a421717be61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of tokens: ['Artificial', 'Intelligence', '(', 'AI', ')', 'is', 'revolutionizing', 'the', 'way', 'we', 'live', 'and', 'work', '.', 'From', 'self-driving', 'cars', 'to', 'virtual', 'assistants', ',', 'AI', 'technologies', 'are', 'becoming', 'increasingly', 'prevalent', '.', 'These', 'innovations', 'are', 'not', 'just', 'enhancing', 'productivity', 'but', 'also', 'improving', 'the', 'quality', 'of', 'life', '.', 'As', 'AI', 'continues', 'to', 'evolve', ',', 'it', 'raises', 'important', 'ethical', 'and', 'social', 'questions', 'that', 'society', 'must', 'address', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Count the number of tokens in the text\n",
        "num_tokens = len(tokens)\n",
        "print(\"Number of tokens:\", num_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSgIDQEbMbLF",
        "outputId": "3036ab02-4df8-4f0e-8f53-6f0ff32b5bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Identify the frequency of each token using nltk.FreqDist\n",
        "frequency_distribution = FreqDist(tokens)\n",
        "# Uses FreqDist to calculate and print the frequency of each token"
      ],
      "metadata": {
        "id": "vJsR9_0SMeyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the frequency of each token\n",
        "print(\"\\nFrequency of each token:\")\n",
        "for token, frequency in frequency_distribution.items():\n",
        "    print(f\"{token}: {frequency}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZUxAZc5MfSc",
        "outputId": "4f944087-c88f-487d-b044-830cc01b88cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Frequency of each token:\n",
            "Artificial: 1\n",
            "Intelligence: 1\n",
            "(: 1\n",
            "AI: 3\n",
            "): 1\n",
            "is: 1\n",
            "revolutionizing: 1\n",
            "the: 2\n",
            "way: 1\n",
            "we: 1\n",
            "live: 1\n",
            "and: 2\n",
            "work: 1\n",
            ".: 4\n",
            "From: 1\n",
            "self-driving: 1\n",
            "cars: 1\n",
            "to: 2\n",
            "virtual: 1\n",
            "assistants: 1\n",
            ",: 2\n",
            "technologies: 1\n",
            "are: 2\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "prevalent: 1\n",
            "These: 1\n",
            "innovations: 1\n",
            "not: 1\n",
            "just: 1\n",
            "enhancing: 1\n",
            "productivity: 1\n",
            "but: 1\n",
            "also: 1\n",
            "improving: 1\n",
            "quality: 1\n",
            "of: 1\n",
            "life: 1\n",
            "As: 1\n",
            "continues: 1\n",
            "evolve: 1\n",
            "it: 1\n",
            "raises: 1\n",
            "important: 1\n",
            "ethical: 1\n",
            "social: 1\n",
            "questions: 1\n",
            "that: 1\n",
            "society: 1\n",
            "must: 1\n",
            "address: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rP6j80TfMiMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}